# OpenAI Realtime API Relay Server

Express.js relay server to proxy WebRTC connections to OpenAI's Realtime API from browsers.

## Why This Server is Needed

OpenAI's Realtime API doesn't support direct browser connections due to CORS restrictions. This relay server:
- Accepts WebRTC SDP offers from browsers
- Forwards them to OpenAI's API on the server side
- Returns the SDP answer back to the browser
- Enables browsers to use OpenAI's Realtime voice features

## Features

- ‚úÖ **Express.js** - Fast, minimal web framework
- ‚úÖ **TypeScript** - Type-safe development
- ‚úÖ **Security** - Helmet, CORS, rate limiting
- ‚úÖ **Production Ready** - PM2, Docker, graceful shutdown
- ‚úÖ **Health Checks** - Monitor server status
- ‚úÖ **Logging** - Request/error logging
- ‚úÖ **Rate Limiting** - Prevent abuse

## Quick Start

### 1. Install Dependencies

```bash
cd openai-relay-server
npm install
```

### 2. Configure Environment

```bash
cp .env.example .env
```

Edit `.env`:
```bash
OPENAI_API_KEY=your_openai_api_key_here
PORT=8080
NODE_ENV=development
ALLOWED_ORIGINS=http://localhost:5173,http://localhost:3000
```

### 3. Run Development Server

```bash
npm run dev
```

Server will start on `http://localhost:8080`

### 4. Test Health Endpoint

```bash
curl http://localhost:8080/health
```

## API Endpoints

### GET /health

Health check endpoint.

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-27T10:00:00.000Z",
  "uptime": 123.456,
  "version": "1.0.0"
}
```

### POST /offer

Relay WebRTC SDP offer to OpenAI.

**Request Body:**
```json
{
  "sdp": "v=0\r\no=- ...",
  "model": "gpt-realtime-mini-2025-10-06",
  "token": "ephemeral_token_from_openai"
}
```

**Response:**
```json
{
  "sdp": "v=0\r\na=group:BUNDLE ...",
  "relayed": true,
  "duration": 234
}
```

## Deployment Options

### üöÄ Quick Deploy (Production)

**Want to deploy quickly?** Follow our **[Quick Deployment Guide](./DEPLOY_QUICK.md)** - get HTTPS relay server running in **under 15 minutes**!

### üìö Deployment Guides

| Guide | Use Case | Time | Link |
|-------|----------|------|------|
| **Quick Deploy** | Fastest production setup with HTTPS | 15 min | [DEPLOY_QUICK.md](./DEPLOY_QUICK.md) |
| **Production Guide** | Complete production deployment | 30 min | [PRODUCTION_DEPLOYMENT_GUIDE.md](./PRODUCTION_DEPLOYMENT_GUIDE.md) |
| **Docker Compose** | Docker Compose commands & tips | Reference | [DOCKER_COMPOSE_GUIDE.md](./DOCKER_COMPOSE_GUIDE.md) |
| **SSH Deployment** | Step-by-step SSH deployment | 45 min | [DEPLOYMENT_SSH.md](./DEPLOYMENT_SSH.md) |

### Option 1: Docker Compose - Recommended (Easiest!)

Docker Compose provides the simplest deployment with configuration management.

```bash
# 1. Configure environment
cp .env.example .env
nano .env  # Add your OPENAI_API_KEY

# 2. Start server (one command!)
docker-compose up -d

# 3. View logs
docker-compose logs -f

# 4. Check status
docker-compose ps

# Management commands
docker-compose restart  # Restart
docker-compose down     # Stop
docker-compose up -d --build  # Update
```

**Why Docker Compose**:
- ‚úÖ **Simplest** - One command to start/stop
- ‚úÖ **Configuration** - Manage via .env file
- ‚úÖ **Auto-restart** - Built-in restart policy
- ‚úÖ **Easy updates** - `docker-compose up -d --build`
- ‚úÖ **Best for production** - Industry standard

**See [DOCKER_COMPOSE_GUIDE.md](./DOCKER_COMPOSE_GUIDE.md) for complete guide**

### Option 2: Docker (Manual)

If you prefer manual Docker commands:

```bash
# Build image
docker build -t openai-relay:latest .

# Run container
docker run -d \
  --name openai-relay \
  -p 8080:8080 \
  -e OPENAI_API_KEY=your_key_here \
  -e ALLOWED_ORIGINS=* \
  --restart unless-stopped \
  openai-relay:latest

# View logs
docker logs -f openai-relay
```

### Option 3: PM2 (Process Manager)

If you prefer running directly on the host without Docker, use PM2.

PM2 provides automatic restarts, monitoring, and log management.

```bash
# Install PM2 globally
npm install -g pm2

# Build the application
npm run build

# Start with PM2
pm2 start ecosystem.config.js

# View status
pm2 status

# View logs
pm2 logs openai-relay

# Restart
pm2 restart openai-relay

# Stop
pm2 stop openai-relay

# Monitor
pm2 monit
```

**Auto-start on system boot:**
```bash
pm2 startup
pm2 save
```

### Option 4: Systemd Service

If you prefer a native Linux service without Docker or PM2:

Create `/etc/systemd/system/openai-relay.service`:

```ini
[Unit]
Description=OpenAI Realtime Relay Server
After=network.target

[Service]
Type=simple
User=nodejs
WorkingDirectory=/opt/openai-relay-server
Environment=NODE_ENV=production
ExecStart=/usr/bin/node /opt/openai-relay-server/dist/index.js
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=openai-relay

[Install]
WantedBy=multi-user.target
```

Enable and start:
```bash
sudo systemctl enable openai-relay
sudo systemctl start openai-relay
sudo systemctl status openai-relay
```

## üß™ Verify Deployment

After deployment, run the verification script:

```bash
./verify-deployment.sh
```

This checks:
- ‚úÖ Docker installation and service
- ‚úÖ Environment configuration
- ‚úÖ Container health status
- ‚úÖ Health endpoint response
- ‚úÖ SSL certificate (if configured)
- ‚úÖ Nginx configuration (if used)

## Production Deployment Guides

- **[Quick Deploy (15 min)](./DEPLOY_QUICK.md)** - Fastest way to production with HTTPS
- **[Complete Production Guide](./PRODUCTION_DEPLOYMENT_GUIDE.md)** - Detailed deployment with monitoring
- **[SSH Deployment](./DEPLOYMENT_SSH.md)** - Step-by-step SSH instructions

## Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `OPENAI_API_KEY` | ‚úÖ Yes | - | Your OpenAI API key |
| `PORT` | No | `8080` | Server port |
| `NODE_ENV` | No | `development` | Environment (`development` or `production`) |
| `ALLOWED_ORIGINS` | No | `http://localhost:5173` | Comma-separated list of allowed CORS origins |

## Security Considerations

### 1. CORS Configuration

**Development** - Allow all origins for easy testing:
```bash
ALLOWED_ORIGINS=*
```

**Production** - Specify exact origins for security:
```bash
ALLOWED_ORIGINS=https://your-app.com,https://admin.your-app.com
```

‚ö†Ô∏è Never use `*` in production for public-facing servers (security risk).

**For detailed CORS configuration, see [CORS_CONFIGURATION.md](./CORS_CONFIGURATION.md)**

### 2. API Key Protection

- Store API key in environment variables, never commit to git
- Use different API keys for development and production
- Rotate keys regularly
- Monitor OpenAI API usage

### 3. Rate Limiting

Default: 100 requests per 15 minutes per IP. Adjust in `src/index.ts` if needed.

### 4. HTTPS

Always use HTTPS in production. Set up with:
- Nginx reverse proxy with Let's Encrypt SSL
- Cloudflare proxy
- Load balancer SSL termination

### 5. Firewall

Only expose necessary ports:
```bash
# Allow SSH (22) and HTTP/HTTPS (80/443)
sudo ufw allow 22
sudo ufw allow 80
sudo ufw allow 443
sudo ufw enable
```

## Monitoring

### PM2 Monitoring

```bash
# Real-time monitoring
pm2 monit

# CPU and memory usage
pm2 list

# Logs
pm2 logs openai-relay --lines 100
```

### Health Checks

Set up external monitoring (UptimeRobot, Pingdom, etc.) to check:
```bash
GET http://your-server.com/health
```

Should return `200 OK` with JSON response.

### Logs

Logs are written to:
- **PM2**: `logs/out.log` and `logs/error.log`
- **Docker**: `docker logs openai-relay`
- **Systemd**: `journalctl -u openai-relay -f`

## Troubleshooting

### Server won't start

1. Check if port is already in use:
```bash
lsof -i :8080
```

2. Verify environment variables:
```bash
cat .env
```

3. Check logs:
```bash
pm2 logs openai-relay --err
```

### CORS errors

1. Verify `ALLOWED_ORIGINS` includes your frontend domain
2. Check browser console for exact origin being blocked
3. Restart server after changing `.env`

### OpenAI API errors

1. Verify API key is valid:
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

2. Check OpenAI API status: https://status.openai.com/
3. Review server logs for error details

### High memory usage

1. Check PM2 memory limit:
```bash
pm2 show openai-relay
```

2. Restart if memory leak suspected:
```bash
pm2 restart openai-relay
```

## Development

### Build

```bash
npm run build
```

Compiles TypeScript to `dist/` directory.

### Type Check

```bash
npm run type-check
```

### Run Production Build Locally

```bash
npm run start:prod
```

## File Structure

```
openai-relay-server/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ index.ts          # Main server code
‚îú‚îÄ‚îÄ dist/                 # Compiled JavaScript (generated)
‚îú‚îÄ‚îÄ logs/                 # PM2 logs (generated)
‚îú‚îÄ‚îÄ package.json          # Dependencies
‚îú‚îÄ‚îÄ tsconfig.json         # TypeScript config
‚îú‚îÄ‚îÄ ecosystem.config.js   # PM2 config
‚îú‚îÄ‚îÄ Dockerfile            # Docker config
‚îú‚îÄ‚îÄ .env.example          # Environment template
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ README.md            # This file
‚îî‚îÄ‚îÄ DEPLOYMENT_SSH.md    # SSH deployment guide
```

## Performance

- **Latency**: ~50-100ms relay overhead
- **Throughput**: Handles 100+ concurrent connections
- **Memory**: ~50MB base usage
- **CPU**: <5% idle, <20% under load (single instance)

Scale horizontally by running multiple instances behind a load balancer.

## License

MIT

## Support

For issues related to:
- **This relay server**: Check logs, try troubleshooting steps above
- **OpenAI API**: https://platform.openai.com/docs
- **WebRTC**: https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API

## Credits

Built for CardStudio platform to enable OpenAI Realtime API in browsers.

