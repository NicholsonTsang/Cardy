<template>
  <div class="ai-assistant">
    <!-- AI Button -->
    <button @click="openModal" class="ai-button">
      <i class="pi pi-comments" />
      <span>Ask AI Assistant</span>
    </button>

    <!-- Modal -->
    <Teleport to="body">
      <div v-if="isModalOpen" class="modal-overlay" @click="closeModal">
        <div class="modal-content" @click.stop>
          <!-- Language Selection Screen -->
          <div v-if="showLanguageSelection" class="language-selection-screen">
            <div class="language-header">
              <button @click="closeModal" class="close-button-top-right">
                <i class="pi pi-times" />
              </button>
              <i class="pi pi-globe language-globe-icon" />
              <h2 class="language-title">Choose Your Language</h2>
              <p class="language-subtitle">Select a language to start chatting</p>
            </div>
            
            <div class="language-grid">
              <button
                v-for="lang in languages"
                :key="lang.code"
                @click="selectLanguage(lang)"
                class="language-option"
              >
                <span class="language-flag">{{ lang.flag }}</span>
                <span class="language-name">{{ lang.name }}</span>
              </button>
            </div>
          </div>

          <!-- Chat Interface (shown after language selection) -->
          <template v-else>
            <!-- Header -->
            <div class="modal-header">
              <div class="header-info">
                <i class="pi pi-comments header-icon" />
                <div>
                  <h3 class="header-title">AI Assistant</h3>
                  <p class="header-subtitle">{{ contentItemName }}</p>
                </div>
              </div>
              <div class="header-actions">
                <!-- Mode Switch Button (Phone icon for realtime) -->
                <button 
                  @click="toggleConversationMode" 
                  class="mode-switch-button"
                  :class="{ 'active': conversationMode === 'realtime' }"
                  :title="conversationMode === 'realtime' ? 'Switch to Chat' : 'Switch to Live Call'"
                >
                  <i :class="conversationMode === 'realtime' ? 'pi pi-comments' : 'pi pi-phone'" />
                </button>
                <button @click="closeModal" class="close-button">
                  <i class="pi pi-times" />
                </button>
              </div>
            </div>

            <!-- Chat Container (Chat Completion Mode) -->
            <div v-if="conversationMode === 'chat-completion'" class="chat-container">
            <!-- Messages -->
            <div ref="messagesContainer" class="messages-container">
              <div 
                v-for="message in messages" 
                :key="message.id"
                class="message"
                :class="message.role"
              >
                <div class="message-avatar" :class="{ 'message-avatar-pulse': message.isStreaming && message.role === 'assistant' }">
                  <i :class="message.role === 'user' ? 'pi pi-user' : 'pi pi-sparkles'" />
                </div>
                <div class="message-content">
                  <div class="message-bubble">
                    <!-- Use the normal chat bubble, but stream content progressively with a cursor -->
                    <p v-if="message.content !== undefined" class="message-text">
                      {{ message.content }}
                      <span v-if="message.isStreaming" class="streaming-cursor">▊</span>
                    </p>
                    <div v-if="message.audio && message.role === 'user'" class="audio-indicator">
                      <i class="pi pi-microphone" />
                      <span>Voice message</span>
                    </div>
                    <!-- Audio playback button for assistant messages - inside bubble -->
                    <div v-if="message.role === 'assistant' && message.content && !message.isStreaming" class="audio-section">
                      <button 
                        @click="playMessageAudio(message)"
                        class="audio-play-button"
                        :class="{ 'playing': currentPlayingMessageId === message.id, 'loading': message.audioLoading }"
                        :disabled="message.audioLoading"
                        :title="message.audioLoading ? 'Generating audio...' : currentPlayingMessageId === message.id ? 'Playing...' : 'Play audio'"
                      >
                        <i v-if="message.audioLoading" class="pi pi-spin pi-spinner" />
                        <i v-else-if="currentPlayingMessageId === message.id" class="pi pi-volume-up" />
                        <i v-else class="pi pi-volume-off" />
                      </button>
                      <!-- Audio hint on first assistant message -->
                      <span v-if="!hasShownAudioHint && messages.indexOf(message) === 0" class="audio-hint">
                        Tap to hear
                      </span>
                    </div>
                  </div>
                  <span class="message-time">{{ formatTime(message.timestamp) }}</span>
                </div>
              </div>

              <!-- Enhanced Typing Indicator with Status (only when no streaming message exists) -->
              <div v-if="isLoading && !hasStreaming" class="message assistant">
                <div class="message-avatar message-avatar-pulse">
                  <i class="pi pi-sparkles" />
                </div>
                <div class="message-content">
                  <div class="typing-indicator-enhanced">
                    <div class="typing-dots">
                      <span></span>
                      <span></span>
                      <span></span>
                    </div>
                    <p v-if="loadingStatus" class="loading-status">{{ loadingStatus }}</p>
                  </div>
                </div>
              </div>

            <!-- Error Message -->
              <div v-if="error" class="error-banner">
              <i class="pi pi-exclamation-triangle" />
              <p>{{ error }}</p>
              </div>
            </div>

            <!-- Input Area -->
            <div class="input-area">
              <!-- Input Container -->
              <div class="input-container">
                <!-- Text Mode: Input with buttons -->
                <template v-if="inputMode === 'text'">
              <button 
                    @click="toggleInputMode" 
                    class="input-icon-button"
                    title="Switch to voice input"
              >
                <i class="pi pi-microphone" />
              </button>
                  
                  <input 
                    v-model="textInput"
                    type="text"
                    placeholder="Type your message..."
                    class="text-input"
                    @keypress.enter="sendTextMessage"
                    :disabled="isLoading"
                  />
                  
                  <button 
                    @click="sendTextMessage"
                    class="input-icon-button send-icon"
                    :disabled="!textInput.trim() || isLoading"
                  >
                    <i class="pi pi-send" />
                  </button>
                </template>

                <!-- Voice Mode: Hold button + switch button inline -->
                <template v-else>
                  <!-- Recording UI (shows above buttons when recording) -->
                  <div v-if="isRecording" class="voice-recording-overlay">
                    <!-- Waveform Visualization -->
                    <div class="waveform-section">
                      <canvas ref="waveformCanvas" class="waveform-canvas"></canvas>
            </div>

                    <!-- Recording Info -->
                    <div class="recording-info-section">
                      <div class="recording-pulse"></div>
                      <span class="recording-duration">{{ recordingDuration }}</span>
                      <span class="recording-status">{{ isCancelZone ? 'Release to cancel' : 'Release to send' }}</span>
                  </div>
                  </div>

                  <!-- Hold to Talk Button (inline with switch button) -->
                  <button
                    @mousedown="handleRecordStart"
                    @mouseup="handleRecordEnd"
                    @mouseleave="handleMouseLeave"
                    @touchstart.prevent="handleRecordStart"
                    @touchend.prevent="handleRecordEnd"
                    @touchmove.prevent="handleTouchMove"
                    class="hold-talk-button"
                    :class="{ recording: isRecording, canceling: isCancelZone }"
                    :disabled="isLoading"
                    ref="recordButton"
                  >
                    <i class="pi pi-microphone"></i>
                    <span>Hold to talk</span>
                </button>
                  
                  <!-- Switch to Text Button (same row) -->
                  <button 
                    @click="toggleInputMode" 
                    class="input-icon-button"
                    title="Switch to text input"
                  >
                    <i class="pi pi-keyboard" />
                  </button>
                </template>
              </div>
            </div>
            </div>

            <!-- Realtime Conversation Mode -->
            <div v-else-if="conversationMode === 'realtime'" class="realtime-container">
              <!-- Connection Status Banner -->
              <div class="realtime-status-banner" :class="`status-${realtimeStatus}`">
                <div class="status-indicator">
                  <div class="status-dot"></div>
                  <span class="status-text">{{ getRealtimeStatusText() }}</span>
                </div>
              </div>

              <!-- Main Realtime UI -->
              <div class="realtime-content">
                <!-- AI Avatar with Waveform -->
                <div class="realtime-avatar-section">
                  <div class="realtime-avatar" :class="{ 
                    'speaking': isRealtimeSpeaking,
                    'listening': isRealtimeConnected && !isRealtimeSpeaking,
                    'connecting': realtimeStatus === 'connecting'
                  }">
                    <div class="avatar-circle">
                      <i class="pi pi-sparkles avatar-icon" />
                    </div>
                    
                    <!-- Waveform Visualization (Placeholder) -->
                    <div v-if="isRealtimeConnected" class="waveform-container">
                      <div class="waveform-bars">
                        <div v-for="i in 20" :key="i" class="waveform-bar" 
                             :style="{ animationDelay: `${i * 0.05}s` }"></div>
                      </div>
                    </div>
                  </div>
                  
                  <!-- Status Text -->
                  <div class="realtime-status-text">
                    <h3 v-if="realtimeStatus === 'disconnected'">Ready to Connect</h3>
                    <h3 v-else-if="realtimeStatus === 'connecting'">Connecting...</h3>
                    <h3 v-else-if="isRealtimeSpeaking">AI is speaking</h3>
                    <h3 v-else-if="isRealtimeConnected">Listening...</h3>
                  </div>
                </div>

                <!-- Live Transcript -->
                <div class="realtime-transcript">
                  <div v-if="messages.length === 0" class="transcript-placeholder">
                    <p>Conversation transcript will appear here</p>
                  </div>
                  <div v-else class="transcript-messages">
                    <div v-for="message in messages" :key="message.id" 
                         class="transcript-message" :class="message.role">
                      <span class="transcript-role">{{ message.role === 'user' ? 'You' : 'AI' }}:</span>
                      <span class="transcript-content">{{ message.content }}</span>
                    </div>
                  </div>
                </div>
              </div>

              <!-- Realtime Controls -->
              <div class="realtime-controls">
                <!-- Connect/Disconnect Button -->
                <button 
                  v-if="!isRealtimeConnected"
                  @click="connectRealtime"
                  class="realtime-connect-button"
                  :disabled="realtimeStatus === 'connecting'"
                >
                  <i class="pi pi-phone" />
                  <span>{{ realtimeStatus === 'connecting' ? 'Connecting...' : 'Start Live Call' }}</span>
                </button>

                <!-- Talk Controls (when connected) -->
                <div v-else class="realtime-talk-controls">
                  <button 
                    @click="disconnectRealtime"
                    class="realtime-disconnect-button"
                  >
                    <i class="pi pi-phone" style="transform: rotate(135deg);" />
                    <span>End Call</span>
                  </button>
                  
                  <div class="realtime-mode-info">
                    <i class="pi pi-info-circle" />
                    <span>Speak naturally - AI will respond in real-time</span>
                  </div>
                </div>
              </div>
            </div>

          </template>
        </div>
      </div>
    </Teleport>

    <!-- Hidden audio element for playback -->
    <audio ref="audioPlayer" style="display: none;"></audio>
  </div>
</template>

<script setup lang="ts">
import { ref, onBeforeUnmount, nextTick, computed, watch } from 'vue'
import { supabase } from '@/lib/supabase'

interface Props {
  contentItemName: string
  contentItemContent: string
  aiMetadata: string
  cardData: {
    card_name: string
    card_description: string
    ai_prompt?: string
  }
}

const props = defineProps<Props>()

interface Message {
  id: string
  role: 'user' | 'assistant'
  content?: string
  audio?: {
    data: string
    format: string
  }
  timestamp: Date
  isStreaming?: boolean
  audioUrl?: string  // Cached audio URL
  audioLoading?: boolean  // Audio generation in progress
  language?: string  // Language code when audio was generated
}

// Languages - GPT-4o supports audio for all major languages
const languages = [
  { code: 'en', name: 'English', flag: '🇺🇸' },
  { code: 'zh-HK', name: '廣東話', flag: '🇭🇰' },      // Cantonese
  { code: 'zh-CN', name: '普通话', flag: '🇨🇳' },      // Mandarin
  { code: 'ja', name: '日本語', flag: '🇯🇵' },        // Japanese
  { code: 'ko', name: '한국어', flag: '🇰🇷' },        // Korean
  { code: 'es', name: 'Español', flag: '🇪🇸' },       // Spanish
  { code: 'fr', name: 'Français', flag: '🇫🇷' },      // French
  { code: 'ru', name: 'Русский', flag: '🇷🇺' },       // Russian
  { code: 'ar', name: 'العربية', flag: '🇸🇦' },       // Arabic
  { code: 'th', name: 'ไทย', flag: '🇹🇭' },           // Thai
]

// Environment (for streaming fetch to Edge Functions)
const supabaseUrl = import.meta.env.VITE_SUPABASE_URL as string
const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY as string

// Conversation modes
type ConversationMode = 'chat-completion' | 'realtime'

// State
const isModalOpen = ref(false)
const showLanguageSelection = ref(true)  // NEW: Show language selection first
const selectedLanguage = ref<typeof languages[0] | null>(null)  // Start with null
const conversationMode = ref<ConversationMode>('chat-completion')  // NEW: Track conversation mode
const messages = ref<Message[]>([])
const textInput = ref('')
const inputMode = ref<'text' | 'voice'>('text')
const isRecording = ref(false)
const isLoading = ref(false)
const loadingStatus = ref('') // NEW: Loading status message
const error = ref<string | null>(null)
const isCancelZone = ref(false)
const recordingDuration = ref('0:00')
const recordButton = ref<HTMLButtonElement | null>(null)
const currentPlayingMessageId = ref<string | null>(null)  // Track which message is playing audio
const hasShownAudioHint = ref(false)  // Track if audio hint was shown

// Realtime mode state
const isRealtimeConnected = ref(false)
const isRealtimeSpeaking = ref(false)
const realtimeStatus = ref<string>('disconnected')
const peerConnection = ref<RTCPeerConnection | null>(null)
const dataChannel = ref<RTCDataChannel | null>(null)
const audioElement = ref<HTMLAudioElement | null>(null)

// Whether there is an in-progress streaming assistant message
const hasStreaming = computed(() => messages.value.some((m: any) => m.isStreaming))

// Audio
const audioPlayer = ref<HTMLAudioElement | null>(null)
const mediaRecorder = ref<MediaRecorder | null>(null)
const audioChunks = ref<Blob[]>([])
const messagesContainer = ref<HTMLElement | null>(null)
const waveformCanvas = ref<HTMLCanvasElement | null>(null)

// Recording state
const recordingStartTime = ref<number>(0)
const recordingTimer = ref<number | null>(null)
const audioContext = ref<AudioContext | null>(null)
const analyser = ref<AnalyserNode | null>(null)
const animationFrame = ref<number | null>(null)
const abortController = ref<AbortController | null>(null)

// System Instructions
const systemInstructions = computed(() => {
  const languageName = selectedLanguage.value?.name || 'English'
  
  return `You are an AI assistant for the content item "${props.contentItemName}" within the digital card "${props.cardData.card_name}".

Your role: Provide helpful information about this specific content item to museum/exhibition visitors.

Content Details:
- Item Name: ${props.contentItemName}
- Item Description: ${props.contentItemContent}

${props.aiMetadata ? `Additional Knowledge: ${props.aiMetadata}` : ''}

${props.cardData.ai_prompt ? `Special Instructions: ${props.cardData.ai_prompt}` : ''}

Communication Guidelines:
- Speak ONLY in ${languageName}
- Be conversational and friendly
- Focus specifically on this content item
- Provide engaging and educational responses
- Keep responses concise but informative (2-3 sentences max for chat)
- If asked about other topics, politely redirect to this content item

Remember: You are here to enhance the visitor's understanding of "${props.contentItemName}".`
})

// Methods
// Welcome messages in all supported languages
const welcomeMessages: Record<string, string> = {
  'en': `Hi! I'm your AI assistant for "${props.contentItemName}". Feel free to ask me anything about this exhibit!`,
  'zh-HK': `你好！我係「${props.contentItemName}」嘅AI助手。有咩想知都可以問我！`,
  'zh-CN': `你好！我是「${props.contentItemName}」的AI助手。有什么想知道的都可以问我！`,
  'ja': `こんにちは！「${props.contentItemName}」のAIアシスタントです。この展示について何でも聞いてください！`,
  'ko': `안녕하세요! "${props.contentItemName}"의 AI 어시스턴트입니다. 이 전시에 대해 무엇이든 물어보세요!`,
  'es': `¡Hola! Soy tu asistente de IA para "${props.contentItemName}". ¡Pregúntame lo que quieras sobre esta exhibición!`,
  'fr': `Bonjour ! Je suis votre assistant IA pour "${props.contentItemName}". N'hésitez pas à me poser des questions sur cette exposition !`,
  'ru': `Привет! Я ваш AI-помощник для "${props.contentItemName}". Спрашивайте меня о чем угодно!`,
  'ar': `مرحبا! أنا مساعدك الذكي لـ "${props.contentItemName}". لا تتردد في طرح أي أسئلة!`,
  'th': `สวัสดี! ฉันเป็น AI ผู้ช่วยของ "${props.contentItemName}" ถามอะไรก็ได้เกี่ยวกับนิทรรศการนี้!`,
  'pt': `Olá! Sou seu assistente de IA para "${props.contentItemName}". Pergunte-me qualquer coisa sobre esta exposição!`
}

function openModal() {
  isModalOpen.value = true
  document.body.style.overflow = 'hidden'
  showLanguageSelection.value = true
  selectedLanguage.value = null
  messages.value = []
  hasShownAudioHint.value = false
}

// Select language and start chat
function selectLanguage(language: typeof languages[0]) {
  selectedLanguage.value = language
  showLanguageSelection.value = false
  
  // Add welcome message in selected language
  const welcomeText = welcomeMessages[language.code] || welcomeMessages['en']
  messages.value = [{
    id: Date.now().toString(),
    role: 'assistant',
    content: welcomeText,
    timestamp: new Date()
  }]
  
  // Scroll to bottom after welcome message
  nextTick(() => {
    scrollToBottom()
  })
}

function closeModal() {
  isModalOpen.value = false
  document.body.style.overflow = ''
  showLanguageSelection.value = true
  selectedLanguage.value = null
  messages.value = []
  textInput.value = ''
  error.value = null
  hasShownAudioHint.value = false
  conversationMode.value = 'chat-completion' // Reset to default mode
  if (isRealtimeConnected.value) {
    disconnectRealtime() // Clean up realtime connection
  }
  if (isRecording.value) {
    isCancelZone.value = true
    if (mediaRecorder.value) {
      mediaRecorder.value.stop()
    }
    cleanupRecording()
  }
}

// Toggle between chat completion and realtime modes
function toggleConversationMode() {
  if (conversationMode.value === 'chat-completion') {
    conversationMode.value = 'realtime'
    // Clear chat messages when switching to realtime
    messages.value = []
  } else {
    conversationMode.value = 'chat-completion'
    // Disconnect realtime if active
    if (isRealtimeConnected.value) {
      disconnectRealtime()
    }
  }
}

// Get realtime status text
function getRealtimeStatusText(): string {
  switch (realtimeStatus.value) {
    case 'disconnected':
      return 'Not Connected'
    case 'connecting':
      return 'Connecting...'
    case 'connected':
      return isRealtimeSpeaking.value ? 'AI Speaking' : 'Listening'
    case 'error':
      return 'Connection Error'
    default:
      return 'Ready'
  }
}

// Real-time connection variables (WebSocket-based)
const realtimeWebSocket = ref<WebSocket | null>(null)
const realtimeMediaStream = ref<MediaStream | null>(null)
const realtimeAudioContext = ref<AudioContext | null>(null)
const realtimeAnalyser = ref<AnalyserNode | null>(null)
const realtimeWaveformData = ref<Uint8Array | null>(null)
const realtimeWaveformAnimationFrame = ref<number | null>(null)
const realtimeAudioPlayer = ref<AudioContext | null>(null)
const realtimeAudioQueue = ref<ArrayBuffer[]>([])
const realtimeActiveSources = ref<AudioBufferSourceNode[]>([])

// Inactivity timeout to prevent prolonged connections
const INACTIVITY_TIMEOUT_MS = 5 * 60 * 1000 // 5 minutes of inactivity
const realtimeInactivityTimer = ref<number | null>(null)
const realtimeLastActivityTime = ref<number>(Date.now())

// Audio processing nodes (need to be cleaned up properly)
const realtimeAudioSource = ref<MediaStreamAudioSourceNode | null>(null)
const realtimeAudioProcessor = ref<ScriptProcessorNode | null>(null)

// Connect to OpenAI Realtime API via WebSocket
async function connectRealtime() {
  if (!selectedLanguage.value) return
  
  // Prevent multiple connections
  if (isRealtimeConnected.value || realtimeStatus.value === 'connecting') {
    console.log('⚠️ Already connected or connecting, skipping...')
    return
  }
  
  // Clean up any existing connection first
  if (realtimeWebSocket.value) {
    console.log('🧹 Cleaning up existing connection...')
    disconnectRealtime()
    // Wait a bit for cleanup to complete
    await new Promise(resolve => setTimeout(resolve, 100))
  }
  
  realtimeStatus.value = 'connecting'
  error.value = null
  
  try {
    console.log('🎤 Starting realtime connection...')
    
    // Step 1: Get ephemeral token and session config
    console.log('📡 Requesting ephemeral token...')
    const { data: sessionData, error: sessionError } = await supabase.functions.invoke(
      'openai-realtime-relay',
      {
        body: {
          language: selectedLanguage.value.code,
          systemPrompt: systemInstructions.value,
          contentItemName: props.contentItemName
        }
      }
    )

    if (sessionError) throw sessionError
    if (!sessionData.success) throw new Error(sessionData.error)

    console.log('✅ Ephemeral token received')
    const { ephemeral_token, session_config } = sessionData

    // Step 2: Request microphone access
    console.log('🎙️ Requesting microphone access...')
    realtimeMediaStream.value = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 24000  // OpenAI Realtime expects 24kHz
      }
    })
    console.log('✅ Microphone access granted')

    // Step 3: Create audio context for input (waveform) and output (playback)
    realtimeAudioContext.value = new AudioContext({ sampleRate: 24000 })
    const source = realtimeAudioContext.value.createMediaStreamSource(realtimeMediaStream.value)
    realtimeAnalyser.value = realtimeAudioContext.value.createAnalyser()
    realtimeAnalyser.value.fftSize = 64
    source.connect(realtimeAnalyser.value)
    realtimeWaveformData.value = new Uint8Array(realtimeAnalyser.value.frequencyBinCount)
    
    realtimeAudioPlayer.value = new AudioContext({ sampleRate: 24000 })
    console.log('✅ Audio contexts created')

    // Step 4: Connect to OpenAI Realtime API via WebSocket
    console.log('🔗 Connecting to OpenAI Realtime WebSocket...')
    const wsUrl = `wss://api.openai.com/v1/realtime?model=${session_config.model}`
    realtimeWebSocket.value = new WebSocket(wsUrl, ['realtime', `openai-insecure-api-key.${ephemeral_token}`, 'openai-beta.realtime-v1'])

    realtimeWebSocket.value.onopen = () => {
      console.log('✅ WebSocket connected')
      
      // Send session configuration
      if (realtimeWebSocket.value) {
        realtimeWebSocket.value.send(JSON.stringify({
          type: 'session.update',
          session: session_config
        }))
      }
      
      // Set connected state
      realtimeStatus.value = 'connected'
      isRealtimeConnected.value = true
      
      // Start inactivity timer (5 minutes)
      startInactivityTimer()
      
      // Add greeting message
      messages.value = [{
        id: Date.now().toString(),
        role: 'assistant',
        content: `Connected! I'm listening in real-time. Start speaking naturally.`,
        timestamp: new Date()
      }]
      
      // Start waveform animation
      startRealtimeWaveformVisualization()
      
      // Start sending audio
      startSendingAudio()
    }

    realtimeWebSocket.value.onmessage = (event) => {
      try {
        const message = JSON.parse(event.data)
        handleRealtimeEvent(message)
      } catch (err) {
        console.error('Failed to parse WebSocket message:', err)
      }
    }

    realtimeWebSocket.value.onerror = (error) => {
      console.error('❌ WebSocket error:', error)
      handleRealtimeDisconnect('WebSocket error')
    }

    realtimeWebSocket.value.onclose = () => {
      console.log('WebSocket closed')
      if (isRealtimeConnected.value) {
        handleRealtimeDisconnect('Connection closed')
      }
    }
    
  } catch (err: any) {
    console.error('❌ Realtime connection error:', err)
    realtimeStatus.value = 'error'
    error.value = err.message || 'Failed to establish real-time connection'
    
    // Clean up on error
    disconnectRealtime()
  }
}

// Start sending audio to OpenAI
function startSendingAudio() {
  if (!realtimeMediaStream.value || !realtimeAudioContext.value || !realtimeWebSocket.value) return
  
  // CRITICAL: Clean up any existing audio processing chain first
  if (realtimeAudioProcessor.value) {
    console.log('🧹 Cleaning up existing audio processor before creating new one')
    try {
      realtimeAudioProcessor.value.disconnect()
      realtimeAudioProcessor.value.onaudioprocess = null
    } catch (err) {
      console.warn('Error disconnecting old processor:', err)
    }
    realtimeAudioProcessor.value = null
  }
  
  if (realtimeAudioSource.value) {
    try {
      realtimeAudioSource.value.disconnect()
    } catch (err) {
      console.warn('Error disconnecting old source:', err)
    }
    realtimeAudioSource.value = null
  }
  
  console.log('🎙️ Starting audio transmission...')
  
  // Create MediaStreamSource and ScriptProcessor for audio capture
  realtimeAudioSource.value = realtimeAudioContext.value.createMediaStreamSource(realtimeMediaStream.value)
  realtimeAudioProcessor.value = realtimeAudioContext.value.createScriptProcessor(4096, 1, 1)
  
  realtimeAudioProcessor.value.onaudioprocess = (event) => {
    if (!isRealtimeConnected.value || !realtimeWebSocket.value || realtimeWebSocket.value.readyState !== WebSocket.OPEN) {
      return
    }
    
    // Get audio data
    const inputData = event.inputBuffer.getChannelData(0)
    
    // Convert to PCM16
    const pcm16 = new Int16Array(inputData.length)
    for (let i = 0; i < inputData.length; i++) {
      const s = Math.max(-1, Math.min(1, inputData[i]))
      pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
    }
    
    // Convert to base64
    const base64Audio = btoa(String.fromCharCode.apply(null, Array.from(new Uint8Array(pcm16.buffer))))
    
    // Send to OpenAI
    realtimeWebSocket.value!.send(JSON.stringify({
      type: 'input_audio_buffer.append',
      audio: base64Audio
    }))
  }
  
  realtimeAudioSource.value.connect(realtimeAudioProcessor.value)
  realtimeAudioProcessor.value.connect(realtimeAudioContext.value.destination)
  
  console.log('✅ Audio processing chain established')
}

// Handle realtime events from OpenAI
function handleRealtimeEvent(event: any) {
  console.log('Realtime event:', event.type)
  
  // Reset inactivity timer on any meaningful event
  if (['conversation.item.created', 'response.audio.delta', 'response.audio_transcript.delta', 'conversation.item.input_audio_transcription.completed'].includes(event.type)) {
    resetInactivityTimer()
  }
  
  switch (event.type) {
    case 'session.created':
    case 'session.updated':
      console.log('✅ Session configured:', event.session)
      break
      
    case 'conversation.item.created':
      // New item in conversation
      if (event.item.type === 'message' && event.item.role === 'assistant') {
        isRealtimeSpeaking.value = true
      }
      break
      
    case 'response.audio.delta':
      // Streaming audio from AI
      if (event.delta && realtimeAudioPlayer.value) {
        playRealtimeAudio(event.delta)
      }
      break
      
    case 'response.audio_transcript.delta':
      // Streaming transcript from AI
      if (event.delta) {
        updateRealtimeTranscript('assistant', event.delta, false)
      }
      break
      
    case 'response.audio_transcript.done':
      // Complete transcript
      if (event.transcript) {
        updateRealtimeTranscript('assistant', event.transcript, true)
      }
      isRealtimeSpeaking.value = false
      break
      
    case 'conversation.item.input_audio_transcription.completed':
      // User's speech transcribed
      if (event.transcript) {
        updateRealtimeTranscript('user', event.transcript, true)
      }
      break
      
    case 'response.done':
      // AI finished responding
      isRealtimeSpeaking.value = false
      break
      
    case 'error':
      console.error('Realtime API error:', event.error)
      error.value = event.error.message
      break
  }
}

// Play audio received from OpenAI
function playRealtimeAudio(base64Audio: string) {
  if (!realtimeAudioPlayer.value) return
  
  try {
    // Decode base64 to PCM16
    const binaryString = atob(base64Audio)
    const bytes = new Uint8Array(binaryString.length)
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i)
    }
    
    // Convert PCM16 to Float32
    const pcm16 = new Int16Array(bytes.buffer)
    const float32 = new Float32Array(pcm16.length)
    for (let i = 0; i < pcm16.length; i++) {
      float32[i] = pcm16[i] / 32768.0
    }
    
    // Create audio buffer
    const audioBuffer = realtimeAudioPlayer.value.createBuffer(1, float32.length, 24000)
    audioBuffer.getChannelData(0).set(float32)
    
    // Play audio
    const source = realtimeAudioPlayer.value.createBufferSource()
    source.buffer = audioBuffer
    source.connect(realtimeAudioPlayer.value.destination)
    
    // Track this source and clean up when done
    realtimeActiveSources.value.push(source)
    source.onended = () => {
      const index = realtimeActiveSources.value.indexOf(source)
      if (index > -1) {
        realtimeActiveSources.value.splice(index, 1)
      }
      // If no more active sources, mark as not speaking
      if (realtimeActiveSources.value.length === 0) {
        isRealtimeSpeaking.value = false
      }
    }
    
    source.start()
    isRealtimeSpeaking.value = true
  } catch (err) {
    console.error('Failed to play audio:', err)
  }
}

// Update transcript with streaming support
function updateRealtimeTranscript(role: 'user' | 'assistant', text: string, isFinal: boolean) {
  if (!text || text.trim() === '') return
  
  // Find existing streaming message or create new one
  const lastMessage = messages.value[messages.value.length - 1]
  
  if (!isFinal && lastMessage && lastMessage.role === role && !lastMessage.timestamp) {
    // Update streaming message
    lastMessage.content = text
  } else if (isFinal) {
    // Finalize or create new message
    if (lastMessage && lastMessage.role === role && !lastMessage.timestamp) {
      // Finalize existing streaming message
      lastMessage.content = text
      lastMessage.timestamp = new Date()
    } else {
      // Create new final message
      messages.value.push({
        id: Date.now().toString(),
        role: role,
        content: text,
        timestamp: new Date()
      })
    }
  } else {
    // Create new streaming message (use placeholder timestamp for now)
    messages.value.push({
      id: Date.now().toString(),
      role: role,
      content: text,
      timestamp: new Date()  // Add timestamp to match Message interface
    })
  }
}

// Start waveform visualization based on real audio (realtime mode)
function startRealtimeWaveformVisualization() {
  if (!realtimeAnalyser.value || !realtimeWaveformData.value) return
  
  const animate = () => {
    if (!isRealtimeConnected.value || !realtimeAnalyser.value || !realtimeWaveformData.value) return
    
    // Get frequency data
    // @ts-ignore - TypeScript strict checking for ArrayBuffer vs SharedArrayBuffer
    realtimeAnalyser.value.getByteFrequencyData(realtimeWaveformData.value)
    
    // Calculate average volume
    let sum = 0
    for (let i = 0; i < realtimeWaveformData.value.length; i++) {
      sum += realtimeWaveformData.value[i]
    }
    const average = sum / realtimeWaveformData.value.length
    
    // Update waveform bars based on audio frequency
    // This is handled by CSS animations, but we could make them dynamic here
    // In future: update waveform bar heights based on frequency data
    
    realtimeWaveformAnimationFrame.value = requestAnimationFrame(animate)
  }
  
  animate()
}

// Disconnect from realtime API
function disconnectRealtime() {
  console.log('🔌 Disconnecting realtime...')
  
  // Clear inactivity timer (IMPORTANT: Stops cost monitoring)
  clearInactivityTimer()
  
  // CRITICAL: Stop audio processing chain FIRST (prevents duplicate audio)
  if (realtimeAudioProcessor.value) {
    console.log('🛑 Stopping audio processor...')
    try {
      realtimeAudioProcessor.value.disconnect()
      realtimeAudioProcessor.value.onaudioprocess = null
    } catch (err) {
      console.warn('Error stopping processor:', err)
    }
    realtimeAudioProcessor.value = null
  }
  
  if (realtimeAudioSource.value) {
    console.log('🛑 Disconnecting audio source...')
    try {
      realtimeAudioSource.value.disconnect()
    } catch (err) {
      console.warn('Error disconnecting source:', err)
    }
    realtimeAudioSource.value = null
  }
  
  // Stop all active audio sources (output)
  if (realtimeActiveSources.value.length > 0) {
    console.log(`🔇 Stopping ${realtimeActiveSources.value.length} active audio sources...`)
    realtimeActiveSources.value.forEach(source => {
      try {
        source.stop()
        source.disconnect()
      } catch (err) {
        // Source might already be stopped
      }
    })
    realtimeActiveSources.value = []
  }
  
  // Stop waveform animation
  if (realtimeWaveformAnimationFrame.value) {
    cancelAnimationFrame(realtimeWaveformAnimationFrame.value)
    realtimeWaveformAnimationFrame.value = null
  }
  
  // Close WebSocket
  if (realtimeWebSocket.value) {
    realtimeWebSocket.value.close()
    realtimeWebSocket.value = null
  }
  
  // Stop media stream
  if (realtimeMediaStream.value) {
    realtimeMediaStream.value.getTracks().forEach(track => track.stop())
    realtimeMediaStream.value = null
  }
  
  // Close audio contexts
  if (realtimeAudioContext.value) {
    realtimeAudioContext.value.close()
    realtimeAudioContext.value = null
  }
  
  if (realtimeAudioPlayer.value) {
    realtimeAudioPlayer.value.close()
    realtimeAudioPlayer.value = null
  }
  
  // Reset state
  isRealtimeConnected.value = false
  isRealtimeSpeaking.value = false
  realtimeStatus.value = 'disconnected'
  realtimeAnalyser.value = null
  realtimeWaveformData.value = null
  realtimeAudioQueue.value = []
  
  // Add goodbye message
  if (messages.value.length > 0) {
    messages.value.push({
      id: Date.now().toString(),
      role: 'assistant',
      content: 'Call ended. Switch back to chat mode or start a new call.',
      timestamp: new Date()
    })
  }
}

// Handle disconnect with error message
function handleRealtimeDisconnect(reason: string) {
  console.log('Realtime disconnect:', reason)
  error.value = reason
  disconnectRealtime()
}

function toggleInputMode() {
  if (isRecording.value) return // Prevent switching while recording
  inputMode.value = inputMode.value === 'text' ? 'voice' : 'text'
}

async function sendTextMessage() {
  if (!textInput.value.trim() || isLoading.value) return

  const userMessage = textInput.value.trim()
  textInput.value = ''

  // Add user message to chat
  addUserMessage(userMessage)

  // Get AI response (text only)
  await getAIResponse(userMessage)
}

// New press-and-hold recording handlers
async function handleRecordStart(event: MouseEvent | TouchEvent) {
  if (isLoading.value || isRecording.value) return

  try {
    console.log('Starting recording...')
    
    // Get microphone access
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    
    // Setup audio context for waveform visualization
    audioContext.value = new (window.AudioContext || (window as any).webkitAudioContext)()
    const source = audioContext.value.createMediaStreamSource(stream)
    analyser.value = audioContext.value.createAnalyser()
    analyser.value.fftSize = 256
    source.connect(analyser.value)
    
    // Start waveform animation
    startWaveformVisualization()
    
    // Setup MediaRecorder
    const mimeType = MediaRecorder.isTypeSupported('audio/mp3') 
      ? 'audio/mp3' 
      : MediaRecorder.isTypeSupported('audio/webm')
      ? 'audio/webm'
      : 'audio/ogg'
    
    mediaRecorder.value = new MediaRecorder(stream, { mimeType })
    audioChunks.value = []

    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data)
      }
    }

    mediaRecorder.value.onstop = async () => {
      // Stop waveform
      stopWaveformVisualization()
      
      // Check if cancelled
      if (isCancelZone.value) {
        console.log('Recording cancelled by user')
        isCancelZone.value = false
      } else {
        // Process the recording
        const audioBlob = new Blob(audioChunks.value, { type: mimeType })
        await processVoiceInput(audioBlob)
      }
      
      // Stop all tracks
      stream.getTracks().forEach(track => track.stop())
      
      // Cleanup audio context
      if (audioContext.value) {
        audioContext.value.close()
        audioContext.value = null
      }
    }

    // Start recording
    mediaRecorder.value.start()
    isRecording.value = true
    recordingStartTime.value = Date.now()
    
    // Start duration timer
    startDurationTimer()
    
    console.log('Recording started with', mimeType)
    
  } catch (err: any) {
    console.error('Microphone error:', err)
    error.value = 'Microphone access is required for voice input'
    cleanupRecording()
  }
}

function handleRecordEnd(event: MouseEvent | TouchEvent) {
  if (!isRecording.value) return
  
  console.log('Recording ended, cancel zone:', isCancelZone.value)
  
  // Stop recording
  if (mediaRecorder.value) {
    mediaRecorder.value.stop()
  }
  
  // Stop timer
  stopDurationTimer()
  
  // Reset state
  isRecording.value = false
}

function handleMouseLeave(event: MouseEvent) {
  // If mouse leaves button while recording, treat as cancel
  if (isRecording.value) {
    isCancelZone.value = true
    handleRecordEnd(event)
  }
}

function handleTouchMove(event: TouchEvent) {
  if (!isRecording.value || !recordButton.value) return
  
  const touch = event.touches[0]
  const buttonRect = recordButton.value.getBoundingClientRect()
  
  // Check if touch moved significantly up (cancel zone)
  const moveUpDistance = buttonRect.bottom - touch.clientY
  
  if (moveUpDistance > 100) {
    isCancelZone.value = true
  } else {
    isCancelZone.value = false
  }
}

function startDurationTimer() {
  stopDurationTimer()
  
  recordingTimer.value = window.setInterval(() => {
    const elapsed = Math.floor((Date.now() - recordingStartTime.value) / 1000)
    const minutes = Math.floor(elapsed / 60)
    const seconds = elapsed % 60
    recordingDuration.value = `${minutes}:${seconds.toString().padStart(2, '0')}`
  }, 100)
}

function stopDurationTimer() {
  if (recordingTimer.value) {
    clearInterval(recordingTimer.value)
    recordingTimer.value = null
  }
  recordingDuration.value = '0:00'
}

function startWaveformVisualization() {
  if (!waveformCanvas.value || !analyser.value) return
  
  const canvas = waveformCanvas.value
  const canvasContext = canvas.getContext('2d')
  if (!canvasContext) return
  
  // Set canvas size
  canvas.width = canvas.offsetWidth * window.devicePixelRatio
  canvas.height = canvas.offsetHeight * window.devicePixelRatio
  canvasContext.scale(window.devicePixelRatio, window.devicePixelRatio)
  
  const bufferLength = analyser.value.frequencyBinCount
  const dataArray = new Uint8Array(bufferLength)
  
  const draw = () => {
    if (!analyser.value || !isRecording.value) return
    
    animationFrame.value = requestAnimationFrame(draw)
    
    analyser.value.getByteFrequencyData(dataArray)
    
    // Clear canvas
    canvasContext.fillStyle = 'rgba(59, 130, 246, 0.1)'
    canvasContext.fillRect(0, 0, canvas.offsetWidth, canvas.offsetHeight)
    
    // Draw waveform bars
    const barWidth = canvas.offsetWidth / bufferLength * 2.5
    let x = 0
    
    for (let i = 0; i < bufferLength; i++) {
      const barHeight = (dataArray[i] / 255) * canvas.offsetHeight * 0.8
      
      // Gradient color based on amplitude
      const hue = 220 // Blue
      const saturation = 70 + (dataArray[i] / 255) * 30
      const lightness = 50 + (dataArray[i] / 255) * 20
      
      canvasContext.fillStyle = `hsl(${hue}, ${saturation}%, ${lightness}%)`
      canvasContext.fillRect(x, canvas.offsetHeight - barHeight, barWidth - 1, barHeight)
      
      x += barWidth
    }
  }
  
  draw()
}

function stopWaveformVisualization() {
  if (animationFrame.value) {
    cancelAnimationFrame(animationFrame.value)
    animationFrame.value = null
  }
}

function cleanupRecording() {
  stopDurationTimer()
  stopWaveformVisualization()
  
  if (audioContext.value) {
    audioContext.value.close()
    audioContext.value = null
  }
  
  isRecording.value = false
  isCancelZone.value = false
}

async function processVoiceInput(audioBlob: Blob) {
  isLoading.value = true
  error.value = null

  try {
    console.log('Processing voice input, blob size:', audioBlob.size, 'type:', audioBlob.type)
    
    // OpenAI only supports 'wav' and 'mp3' formats
    // Most browsers record in 'webm', so we need to convert
    let finalAudioBlob = audioBlob
    let audioFormat = 'wav'  // Default to wav for OpenAI compatibility
    
    // Check if we need to convert the audio format
    if (audioBlob.type.includes('mp3')) {
      audioFormat = 'mp3'
    } else if (audioBlob.type.includes('wav')) {
      audioFormat = 'wav'
    } else {
      // Convert webm/ogg/other formats to wav using Web Audio API
      console.log('Converting audio from', audioBlob.type, 'to wav format')
      try {
        finalAudioBlob = await convertAudioToWav(audioBlob)
        audioFormat = 'wav'
        console.log('Audio converted to wav, new size:', finalAudioBlob.size)
      } catch (convertError) {
        console.error('Audio conversion failed:', convertError)
        throw new Error('Failed to convert audio format. Please try again.')
      }
    }
    
    // Convert audio blob to base64
    const audioBase64 = await blobToBase64(finalAudioBlob)
    
    console.log('Audio ready for API, format:', audioFormat, 'base64 length:', audioBase64.length)
    
    // Get AI response with voice input
    // The Edge Function will return both the user's transcription and AI's response
    await getAIResponseWithVoice({
      data: audioBase64,
      format: audioFormat
    })

  } catch (err: any) {
    console.error('Voice processing error:', err)
    error.value = err.message || 'Failed to process voice input'
  } finally {
    isLoading.value = false
  }
}

async function getAIResponse(userInput: string) {
  isLoading.value = true
  loadingStatus.value = 'Processing your message...'
  error.value = null

  try {
    console.log('Getting AI response for text input:', userInput.substring(0, 50))
    
    // Prepare conversation history (only content, no audio)
    const conversationMessages = messages.value
      .filter(msg => msg.content)  // Only messages with text content
      .map(msg => ({
        role: msg.role,
        content: msg.content
      }))

    console.log('Conversation history:', conversationMessages.length, 'messages')

    loadingStatus.value = 'Generating response...'

    // Create placeholder assistant message that we will stream into
    const assistantId = Date.now().toString()
    messages.value.push({
      id: assistantId,
      role: 'assistant',
      content: '',
      isStreaming: true,
      timestamp: new Date()
    })

    // Stream from dedicated SSE function
    const response = await fetch(`${supabaseUrl}/functions/v1/chat-with-audio-stream`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${supabaseAnonKey}`,
        'apikey': supabaseAnonKey,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        messages: conversationMessages,
        systemPrompt: systemInstructions.value,
        language: selectedLanguage.value?.code || 'en'
      })
    })

    if (!response.ok || !response.body) {
      throw new Error(`Failed to stream: ${response.status}`)
    }

    const reader = response.body.getReader()
    const decoder = new TextDecoder()
    let fullContent = ''

    while (true) {
      const { done, value } = await reader.read()
      if (done) break

      const chunk = decoder.decode(value)
      const lines = chunk.split('\n\n')

      for (const line of lines) {
        if (!line.startsWith('data: ')) continue
        const data = line.slice(6)
        if (data === '[DONE]') continue
        try {
          const parsed = JSON.parse(data)
          if (parsed.content) {
            fullContent += parsed.content
            const idx = messages.value.findIndex(m => m.id === assistantId)
            if (idx !== -1) {
              messages.value[idx].content = fullContent
            }
            await scrollToBottom()
          }
        } catch (_) {
          // ignore partial/invalid json chunks
        }
      }
    }

    // Mark as finished
    const idx = messages.value.findIndex(m => m.id === assistantId)
    if (idx !== -1) {
      messages.value[idx].isStreaming = false
    }

  } catch (err: any) {
    console.error('AI response error:', err)
    error.value = err.message || 'Failed to get AI response'
  } finally {
    isLoading.value = false
    loadingStatus.value = ''
  }
}

async function getAIResponseWithVoice(voiceInput: { data: string, format: string }) {
  if (!selectedLanguage.value) return
  
  isLoading.value = true
  loadingStatus.value = 'Processing voice input...'
  error.value = null

  try {
    console.log('Getting AI response for voice input (Audio Model with transcription display)')
    
    // Prepare conversation history (text only, no audio)
    const conversationMessages = messages.value
      .filter(msg => msg.content && !msg.content.includes('[Processing voice message...]'))
      .map(msg => ({
        role: msg.role,
        content: msg.content
      }))

    console.log('Using audio model for STT + Text generation...')
    loadingStatus.value = 'Processing your voice...'

    // Use audio model for STT + text generation (single call)
    // The Edge Function will return both user transcription and AI response
    const { data, error: funcError } = await supabase.functions.invoke('chat-with-audio', {
      body: {
        messages: conversationMessages,
        systemPrompt: systemInstructions.value,
        language: selectedLanguage.value.code,
        modalities: ['text'],  // Text only output (no audio from model)
        voiceInput: voiceInput
      }
    })

    if (funcError) throw funcError
    if (!data.success) throw new Error(data.error)

    console.log('Received response with transcription:', data)

    // Extract user transcription (what the user said)
    const userTranscription = data.userTranscription || '[Voice input]'
    console.log('User said:', userTranscription)

    // Extract AI text content from response
    let textContent = ''
    if (typeof data.message.content === 'string') {
      textContent = data.message.content
    } else if (Array.isArray(data.message.content)) {
      // Handle array content (OpenAI might return array format)
      const textPart = data.message.content.find((part: any) => part.type === 'text')
      textContent = textPart?.text || ''
    } else if (data.message.audio?.transcript) {
      // Fallback: use transcript if content is empty
      textContent = data.message.audio.transcript
    }

    if (!textContent) {
      console.error('No text content in voice response:', data.message)
      throw new Error('No text content in AI voice response')
    }

    // Add user's transcribed message first (what they said)
    addUserMessage(userTranscription)

    // Add assistant message (text immediately visible, no audio yet)
    addAssistantMessage(textContent)
    
    // Don't auto-generate audio - let user click button to play

  } catch (err: any) {
    console.error('AI voice response error:', err)
    error.value = err.message || 'Failed to get AI response'
  } finally {
    isLoading.value = false
    loadingStatus.value = ''
  }
}

// New function: Generate and play TTS audio
// Generate TTS audio and play it (with caching)
async function generateAndPlayTTS(text: string, messageId: string) {
  if (!selectedLanguage.value) return
  
  try {
    // Find the message
    const message = messages.value.find(m => m.id === messageId)
    if (!message) {
      console.error('Message not found:', messageId)
      return
    }

    // If audio is cached AND language hasn't changed, just play it
    if (message.audioUrl && message.language === selectedLanguage.value.code) {
      console.log('Playing cached audio for message:', messageId)
      await playAudioUrl(message.audioUrl, messageId)
      return
    }
    
    // If language changed, clear cached audio and regenerate
    if (message.audioUrl && message.language !== selectedLanguage.value.code) {
      console.log('Language changed, regenerating audio:', message.language, '→', selectedLanguage.value.code)
      URL.revokeObjectURL(message.audioUrl)
      message.audioUrl = undefined
      message.language = undefined
    }

    // Mark as loading
    message.audioLoading = true

    console.log('Generating TTS audio for text:', text.substring(0, 50))
    
    // Call TTS Edge Function
    const response = await fetch(`${supabaseUrl}/functions/v1/generate-tts-audio`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${supabaseAnonKey}`,
        'apikey': supabaseAnonKey,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        text: text,
        language: selectedLanguage.value.code,
        voice: 'alloy'  // Can be made configurable later
      })
    })

    if (!response.ok) {
      const errorData = await response.json()
      throw new Error(errorData.error || `TTS failed: ${response.status}`)
    }

    // Get audio blob
    const audioBlob = await response.blob()
    console.log('TTS audio generated, size:', audioBlob.size)

    // Create and cache audio URL with language tag
    const audioUrl = URL.createObjectURL(audioBlob)
    message.audioUrl = audioUrl
    message.language = selectedLanguage.value.code
    console.log('Audio cached for message:', messageId, 'in language:', message.language)

    // Mark as loaded
    message.audioLoading = false

    // Play audio
    await playAudioUrl(audioUrl, messageId)

  } catch (err: any) {
    console.error('TTS generation error:', err)
    // Mark as loaded even on error
    const message = messages.value.find(m => m.id === messageId)
    if (message) {
      message.audioLoading = false
    }
    throw err
  }
}

// Play audio from URL
async function playAudioUrl(audioUrl: string, messageId: string) {
  try {
    currentPlayingMessageId.value = messageId
    
    if (audioPlayer.value) {
      audioPlayer.value.src = audioUrl
      await audioPlayer.value.play()
      
      // Clear playing state when done
      audioPlayer.value.onended = () => {
        currentPlayingMessageId.value = null
      }
      
      // Handle errors
      audioPlayer.value.onerror = () => {
        currentPlayingMessageId.value = null
        console.error('Audio playback error')
      }
    }
  } catch (err: any) {
    currentPlayingMessageId.value = null
    console.error('Audio playback error:', err)
    throw err
  }
}

// Play message audio (triggered by button click)
async function playMessageAudio(message: Message) {
  try {
    if (!message.content) return
    
    // If already playing this message, stop it
    if (currentPlayingMessageId.value === message.id) {
      if (audioPlayer.value) {
        audioPlayer.value.pause()
        audioPlayer.value.currentTime = 0
        currentPlayingMessageId.value = null
      }
      return
    }
    
    // Stop any currently playing audio
    if (audioPlayer.value && currentPlayingMessageId.value) {
      audioPlayer.value.pause()
      audioPlayer.value.currentTime = 0
    }
    
    // Generate and play TTS
    await generateAndPlayTTS(message.content, message.id)
    
  } catch (err: any) {
    console.error('Play message audio error:', err)
    error.value = 'Failed to play audio'
  }
}

// Helper: Convert audio blob to WAV format using Web Audio API
async function convertAudioToWav(audioBlob: Blob): Promise<Blob> {
  // Create audio context
  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
  
  // Read audio blob as array buffer
  const arrayBuffer = await audioBlob.arrayBuffer()
  
  // Decode audio data
  const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
  
  // Convert to WAV format
  const wavBlob = audioBufferToWav(audioBuffer)
  
  // Close audio context to free resources
  await audioContext.close()
  
  return wavBlob
}

// Helper: Convert AudioBuffer to WAV Blob
function audioBufferToWav(audioBuffer: AudioBuffer): Blob {
  const numberOfChannels = audioBuffer.numberOfChannels
  const sampleRate = audioBuffer.sampleRate
  const format = 1 // PCM
  const bitDepth = 16
  
  const bytesPerSample = bitDepth / 8
  const blockAlign = numberOfChannels * bytesPerSample
  
  const data: Float32Array[] = []
  for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
    data.push(audioBuffer.getChannelData(i))
  }
  
  const interleaved = interleaveChannels(data)
  const dataLength = interleaved.length * bytesPerSample
  const buffer = new ArrayBuffer(44 + dataLength)
  const view = new DataView(buffer)
  
  // Write WAV header
  writeString(view, 0, 'RIFF')
  view.setUint32(4, 36 + dataLength, true)
  writeString(view, 8, 'WAVE')
  writeString(view, 12, 'fmt ')
  view.setUint32(16, 16, true) // fmt chunk size
  view.setUint16(20, format, true)
  view.setUint16(22, numberOfChannels, true)
  view.setUint32(24, sampleRate, true)
  view.setUint32(28, sampleRate * blockAlign, true) // byte rate
  view.setUint16(32, blockAlign, true)
  view.setUint16(34, bitDepth, true)
  writeString(view, 36, 'data')
  view.setUint32(40, dataLength, true)
  
  // Write audio data
  let offset = 44
  for (let i = 0; i < interleaved.length; i++) {
    const sample = Math.max(-1, Math.min(1, interleaved[i]))
    view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true)
    offset += 2
  }
  
  return new Blob([buffer], { type: 'audio/wav' })
}

// Helper: Interleave multiple audio channels
function interleaveChannels(channels: Float32Array[]): Float32Array {
  const length = channels[0].length
  const numberOfChannels = channels.length
  const result = new Float32Array(length * numberOfChannels)
  
  for (let i = 0; i < length; i++) {
    for (let channel = 0; channel < numberOfChannels; channel++) {
      result[i * numberOfChannels + channel] = channels[channel][i]
    }
  }
  
  return result
}

// Helper: Write string to DataView
function writeString(view: DataView, offset: number, string: string) {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i))
  }
}

// Helper: Convert Blob to Base64
async function blobToBase64(blob: Blob): Promise<string> {
  return new Promise((resolve, reject) => {
    const reader = new FileReader()
    reader.onloadend = () => {
      const base64 = reader.result as string
      // Remove data URL prefix (data:audio/webm;base64,)
      const base64Data = base64.split(',')[1]
      resolve(base64Data)
    }
    reader.onerror = reject
    reader.readAsDataURL(blob)
  })
}

// Helper: Play audio from base64
async function playAudioFromBase64(base64Audio: string, format: string) {
  try {
    const audioBlob = base64ToBlob(base64Audio, `audio/${format}`)
    const audioUrl = URL.createObjectURL(audioBlob)
    
    if (audioPlayer.value) {
      audioPlayer.value.src = audioUrl
      await audioPlayer.value.play()
      
      // Clean up URL after playback
      audioPlayer.value.onended = () => {
        URL.revokeObjectURL(audioUrl)
      }
    }
    } catch (err) {
    console.error('Audio playback error:', err)
  }
}

// Helper: Convert base64 to Blob
function base64ToBlob(base64: string, mimeType: string): Blob {
  const byteCharacters = atob(base64)
  const byteNumbers = new Array(byteCharacters.length)
  for (let i = 0; i < byteCharacters.length; i++) {
    byteNumbers[i] = byteCharacters.charCodeAt(i)
  }
  const byteArray = new Uint8Array(byteNumbers)
  return new Blob([byteArray], { type: mimeType })
}

function addUserMessage(content: string) {
  messages.value.push({
    id: Date.now().toString(),
    role: 'user',
    content,
    timestamp: new Date()
  })
  scrollToBottom()
}

function addAssistantMessage(content: string) {
  messages.value.push({
    id: Date.now().toString(),
    role: 'assistant',
    content,
    timestamp: new Date()
  })
  scrollToBottom()
}

async function scrollToBottom() {
  await nextTick()
  if (messagesContainer.value) {
    messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
  }
}

function formatTime(date: Date): string {
  return date.toLocaleTimeString('en-US', { 
    hour: 'numeric', 
    minute: '2-digit',
    hour12: true 
  })
}

// Cleanup
// Watch for language changes and update the first message if it exists
watch(selectedLanguage, (newLang, oldLang) => {
  if (messages.value.length > 0 && messages.value[0].role === 'assistant') {
    // Update the welcome message to the new language
    if (selectedLanguage.value) {
      const welcomeText = welcomeMessages[selectedLanguage.value.code] || welcomeMessages['en']
      messages.value[0].content = welcomeText
    }
  }
})

// Lifecycle: Cleanup on component unmount
onBeforeUnmount(() => {
  console.log('🧹 Component unmounting - cleaning up all connections')
  
  // Disconnect realtime if active (CRITICAL: Prevents background API costs)
  if (isRealtimeConnected.value) {
    console.log('⚠️ Realtime connection active during unmount - disconnecting to prevent background costs')
    disconnectRealtime()
  }
  
  cleanupRecording()
  document.body.style.overflow = ''
  
  // Remove all event listeners
  if (typeof window !== 'undefined') {
    window.removeEventListener('beforeunload', handleBeforeUnload)
    document.removeEventListener('visibilitychange', handleVisibilityChange)
    window.removeEventListener('blur', handleWindowBlur)
  }
})

// Page Visibility: Disconnect when tab is hidden (CRITICAL for cost control)
function handleVisibilityChange() {
  if (document.hidden && isRealtimeConnected.value) {
    console.log('⚠️ Tab hidden with active realtime connection - disconnecting to save costs')
    disconnectRealtime()
    error.value = 'Connection closed: Tab was hidden. Please reconnect when ready.'
  }
}

// Window Blur: Disconnect when window loses focus (additional safeguard)
function handleWindowBlur() {
  // Small delay to prevent false positives (e.g., clicking inspector)
  setTimeout(() => {
    if (isRealtimeConnected.value && document.hidden) {
      console.log('⚠️ Window lost focus with hidden tab - disconnecting realtime')
      disconnectRealtime()
    }
  }, 1000)
}

// Before Unload: Disconnect when user closes/refreshes page
function handleBeforeUnload(e: BeforeUnloadEvent) {
  if (isRealtimeConnected.value) {
    console.log('⚠️ Page unload with active realtime connection - disconnecting')
    disconnectRealtime()
    
    // Warn user they have an active connection
    const message = 'You have an active AI voice call. Are you sure you want to leave?'
    e.preventDefault()
    e.returnValue = message
    return message
  }
}

// Inactivity Timeout: Disconnect after 5 minutes of no activity
function startInactivityTimer() {
  // Clear existing timer
  if (realtimeInactivityTimer.value) {
    clearTimeout(realtimeInactivityTimer.value)
  }
  
  realtimeLastActivityTime.value = Date.now()
  
  realtimeInactivityTimer.value = window.setTimeout(() => {
    if (isRealtimeConnected.value) {
      console.log('⚠️ Inactivity timeout reached (5 minutes) - disconnecting to save costs')
      disconnectRealtime()
      error.value = 'Connection closed due to inactivity. Please start a new call when ready.'
    }
  }, INACTIVITY_TIMEOUT_MS)
  
  console.log('⏱️ Inactivity timer started (5 minutes)')
}

function resetInactivityTimer() {
  const now = Date.now()
  const timeSinceLastActivity = now - realtimeLastActivityTime.value
  
  // Only reset if it's been at least 10 seconds since last reset (debounce)
  if (timeSinceLastActivity > 10000) {
    startInactivityTimer()
    console.log('⏱️ Inactivity timer reset')
  }
}

function clearInactivityTimer() {
  if (realtimeInactivityTimer.value) {
    clearTimeout(realtimeInactivityTimer.value)
    realtimeInactivityTimer.value = null
    console.log('⏱️ Inactivity timer cleared')
  }
}

// Register event listeners when modal opens
function registerSafeguards() {
  if (typeof window !== 'undefined') {
    window.addEventListener('beforeunload', handleBeforeUnload)
    document.addEventListener('visibilitychange', handleVisibilityChange)
    window.addEventListener('blur', handleWindowBlur)
    console.log('🛡️ Cost safeguards registered')
  }
}

// Call registerSafeguards when component is set up
if (typeof window !== 'undefined') {
  registerSafeguards()
}
</script>

<style scoped>
/* AI Button */
.ai-button {
  position: relative;
  width: 100%;
  padding: 0.875rem 1.25rem;
  background: linear-gradient(135deg, #3b82f6, #6366f1);
  border: none;
  border-radius: 0.75rem;
  color: white;
  font-size: 0.875rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.5rem;
  transition: all 0.2s;
  box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
}

.ai-button:active {
  transform: scale(0.98);
}

/* Modal */
.modal-overlay {
  position: fixed;
  inset: 0;
  z-index: 9999;
  background: rgba(0, 0, 0, 0.85);
  backdrop-filter: blur(4px);
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 1rem;
}

.modal-content {
  width: 100%;
  max-width: 28rem;
  max-height: 90vh;
  background: white;
  border-radius: 1rem;
  overflow: hidden;
  display: flex;
  flex-direction: column;
  animation: slideUp 0.3s ease-out;
  position: relative;
}

/* Language Selection Screen */
.language-selection-screen {
  padding: 2rem 1.5rem;
  display: flex;
  flex-direction: column;
  gap: 2rem;
  min-height: 400px;
  max-height: 90vh;
  overflow-y: auto;
  overflow-x: hidden;
}

.language-header {
  text-align: center;
  position: relative;
}

.close-button-top-right {
  position: absolute;
  top: -0.5rem;
  right: -0.5rem;
  background: transparent;
  border: none;
  cursor: pointer;
  font-size: 1.5rem;
  color: #9ca3af;
  transition: color 0.2s;
  padding: 0.5rem;
}

.close-button-top-right:hover {
  color: #374151;
}

.language-globe-icon {
  font-size: 3rem;
  color: #3b82f6;
  margin-bottom: 1rem;
}

.language-title {
  font-size: 1.5rem;
  font-weight: 700;
  color: #111827;
  margin: 0 0 0.5rem 0;
}

.language-subtitle {
  font-size: 0.875rem;
  color: #6b7280;
  margin: 0;
}

.language-grid {
  display: grid;
  grid-template-columns: repeat(2, 1fr);
  gap: 0.75rem;
}

.language-option {
  background: linear-gradient(135deg, #f9fafb, #f3f4f6);
  border: 2px solid #e5e7eb;
  border-radius: 0.75rem;
  padding: 1.25rem 1rem;
  cursor: pointer;
  transition: all 0.2s ease;
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 0.5rem;
  min-height: 5rem;
}

.language-option:hover {
  background: linear-gradient(135deg, #eff6ff, #dbeafe);
  border-color: #3b82f6;
  transform: translateY(-2px);
  box-shadow: 0 4px 6px -1px rgba(59, 130, 246, 0.1);
}

.language-option:active {
  transform: translateY(0);
}

.language-flag {
  font-size: 2rem;
  line-height: 1;
}

.language-name {
  font-size: 0.875rem;
  font-weight: 600;
  color: #374151;
  text-align: center;
}

/* Modal Header */
.modal-header {
  background: linear-gradient(135deg, #3b82f6, #6366f1);
  padding: 1rem;
  display: flex;
  align-items: center;
  justify-content: space-between;
  flex-shrink: 0;
}

.header-actions {
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.mode-switch-button {
  width: 2rem;
  height: 2rem;
  background: rgba(255, 255, 255, 0.15);
  border: 1px solid rgba(255, 255, 255, 0.3);
  border-radius: 50%;
  color: white;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.2s;
  cursor: pointer;
  font-size: 0.875rem;
}

.mode-switch-button:hover {
  background: rgba(255, 255, 255, 0.25);
  transform: scale(1.05);
}

.mode-switch-button.active {
  background: rgba(16, 185, 129, 0.3);
  border-color: rgba(16, 185, 129, 0.5);
}

.header-info {
  display: flex;
  align-items: center;
  gap: 0.75rem;
}

.header-icon {
  width: 2rem;
  height: 2rem;
  background: rgba(255, 255, 255, 0.2);
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  color: white;
}

.header-title {
  font-size: 1rem;
  font-weight: 600;
  color: white;
  margin: 0;
}

.header-subtitle {
  font-size: 0.75rem;
  color: rgba(255, 255, 255, 0.8);
  margin: 0;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
  max-width: 14rem;
}

.close-button {
  width: 2rem;
  height: 2rem;
  background: rgba(255, 255, 255, 0.2);
  border: none;
  border-radius: 50%;
  color: white;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.2s;
}

.close-button:active {
  background: rgba(255, 255, 255, 0.3);
}

/* Chat Container */
.chat-container {
  display: flex;
  flex-direction: column;
  flex: 1;
  min-height: 0;
}

/* Messages Container */
.messages-container {
  flex: 1;
  overflow-y: auto;
  padding: 1rem;
  display: flex;
  flex-direction: column;
  gap: 1rem;
  background: #f9fafb;
}

/* Message */
.message {
  display: flex;
  gap: 0.75rem;
  animation: messageSlide 0.3s ease-out;
}

.message.user {
  flex-direction: row-reverse;
}

.message-avatar {
  width: 2rem;
  height: 2rem;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  font-size: 0.875rem;
}

.message.user .message-avatar {
  background: linear-gradient(135deg, #3b82f6, #6366f1);
  color: white;
}

.message.assistant .message-avatar {
  background: linear-gradient(135deg, #10b981, #059669);
  color: white;
}

.message-content {
  max-width: 70%;
  display: flex;
  flex-direction: column;
  gap: 0.25rem;
}

.message.user .message-content {
  align-items: flex-end;
}

.message-bubble {
  background: white;
  padding: 0.75rem 1rem;
  border-radius: 1rem;
  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);
  display: flex;
  flex-direction: column;
  gap: 0.35rem;
}

.message-text {
  font-size: 0.875rem;
  line-height: 1.5;
  color: #111827;
  margin: 0;
  word-wrap: break-word;
}

.message.user .message-bubble {
  background: linear-gradient(135deg, #3b82f6, #6366f1);
  color: white;
  border-bottom-right-radius: 0.25rem;
}

.message.user .message-text {
  color: white;
}

.message.assistant .message-bubble {
  border-bottom-left-radius: 0.25rem;
}

.audio-indicator {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  padding: 0.5rem 0.75rem;
  background: rgba(59, 130, 246, 0.1);
  border-radius: 0.5rem;
  font-size: 0.75rem;
  color: #3b82f6;
}

.message-time {
  font-size: 0.625rem;
  color: #9ca3af;
  padding: 0 0.5rem;
}

/* Audio play button - minimal icon only */
.audio-play-button {
  background: transparent;
  border: none;
  padding: 0;
  margin: 0;
  cursor: pointer;
  color: #10b981;
  transition: all 0.2s ease;
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: auto;
  height: auto;
}

.audio-play-button:hover:not(:disabled) {
  color: #059669;
  transform: scale(1.15);
}

.audio-play-button:active:not(:disabled) {
  transform: scale(0.95);
}

.audio-play-button.playing {
  color: #059669;
  animation: pulse 2s ease-in-out infinite;
}

.audio-play-button.loading {
  color: #9ca3af;
  cursor: not-allowed;
}

.audio-play-button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.audio-play-button i {
  font-size: 1rem;
}

/* Audio section with hint */
.audio-section {
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.audio-hint {
  font-size: 0.75rem;
  color: #10b981;
  font-weight: 500;
  animation: fadeIn 0.5s ease-in;
}

/* Enhanced Typing Indicator */
.message-avatar-pulse {
  animation: avatarPulse 2s ease-in-out infinite;
}

@keyframes avatarPulse {
  0%, 100% {
    transform: scale(1);
    box-shadow: 0 0 0 0 rgba(16, 185, 129, 0.4);
  }
  50% {
    transform: scale(1.05);
    box-shadow: 0 0 0 10px rgba(16, 185, 129, 0);
  }
}

.typing-indicator-enhanced {
  background: white;
  padding: 0.75rem 1rem;
  border-radius: 1rem;
  border-bottom-left-radius: 0.25rem;
  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);
  display: flex;
  flex-direction: column;
  gap: 0.5rem;
  min-width: 120px;
}

.typing-dots {
  display: flex;
  gap: 0.25rem;
  align-items: center;
}

.typing-dots span {
  width: 0.5rem;
  height: 0.5rem;
  background: linear-gradient(135deg, #10b981, #059669);
  border-radius: 50%;
  animation: typing 1.4s infinite;
}

.typing-dots span:nth-child(2) {
  animation-delay: 0.2s;
}

.typing-dots span:nth-child(3) {
  animation-delay: 0.4s;
}

.loading-status {
  font-size: 0.75rem;
  color: #059669;
  margin: 0;
  font-weight: 500;
  animation: fadeIn 0.3s ease-in;
}

/* Error Banner */
.error-banner {
  background: #fee2e2;
  border: 1px solid #fecaca;
  border-radius: 0.5rem;
  padding: 0.75rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  color: #991b1b;
  font-size: 0.75rem;
}

/* Input Area */
.input-area {
  background: white;
  border-top: 1px solid #e5e7eb;
  padding: 1rem;
  flex-shrink: 0;
  display: flex;
  flex-direction: column;
  gap: 0.75rem;
}

.language-selector {
  display: flex;
  justify-content: center;
}

.language-dropdown {
  padding: 0.5rem 0.75rem;
  border: 1px solid #d1d5db;
  border-radius: 0.5rem;
  font-size: 0.75rem;
  background: white;
  color: #374151;
}

.input-container {
  position: relative;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  padding: 0.75rem;
  background: white;
  border-top: 1px solid #e5e7eb;
}

/* Input Icon Buttons */
.input-icon-button {
  width: 2.5rem;
  height: 2.5rem;
  border-radius: 50%;
  background: #f3f4f6;
  border: none;
  color: #6b7280;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.2s;
  flex-shrink: 0;
  cursor: pointer;
}

.input-icon-button:hover:not(:disabled) {
  background: #e5e7eb;
}

.input-icon-button:active:not(:disabled) {
  transform: scale(0.95);
}

.input-icon-button.send-icon:not(:disabled) {
  background: linear-gradient(135deg, #3b82f6, #6366f1);
  color: white;
}

.input-icon-button:disabled {
  background: #f3f4f6;
  color: #d1d5db;
  cursor: not-allowed;
  opacity: 0.5;
}

.text-input {
  flex: 1;
  padding: 0.75rem 1rem;
  border: 1px solid #d1d5db;
  border-radius: 1.5rem;
  font-size: 0.875rem;
  outline: none;
  transition: border-color 0.2s;
}

.text-input:focus {
  border-color: #3b82f6;
}

.text-input:disabled {
  background: #f3f4f6;
  color: #9ca3af;
}

/* Hold to Talk Button (inline with keyboard button) */
.hold-talk-button {
  flex: 1;
  padding: 0.75rem 1rem;
  background: white;
  border: 1px solid #d1d5db;
  border-radius: 1.5rem;
  font-size: 0.875rem;
  font-weight: 500;
  color: #374151;
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.5rem;
  transition: all 0.2s;
  cursor: pointer;
  user-select: none;
  -webkit-user-select: none;
  touch-action: none;
}

.hold-talk-button:active:not(:disabled) {
  background: #f3f4f6;
  transform: scale(0.98);
}

.hold-talk-button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.hold-talk-button.recording {
  background: linear-gradient(135deg, #eff6ff, #dbeafe);
  border-color: #3b82f6;
  color: #3b82f6;
}

.hold-talk-button.canceling {
  background: #fee2e2;
  border-color: #ef4444;
  color: #ef4444;
}

/* Voice Recording Overlay (shows above buttons when recording) */
.voice-recording-overlay {
  position: absolute;
  bottom: 100%;
  left: 0;
  right: 0;
  margin-bottom: 0.5rem;
  background: linear-gradient(135deg, #eff6ff, #dbeafe);
  border: 2px solid #3b82f6;
  border-radius: 1rem;
  padding: 1rem;
  display: flex;
  flex-direction: column;
  gap: 0.75rem;
  box-shadow: 0 -4px 12px rgba(0, 0, 0, 0.1);
  animation: slideUpFade 0.2s ease-out;
}

.waveform-section {
  width: 100%;
  height: 60px;
  background: rgba(255, 255, 255, 0.6);
  border-radius: 0.5rem;
  overflow: hidden;
}

.waveform-canvas {
  width: 100%;
  height: 100%;
}

.recording-info-section {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.75rem;
}

.recording-pulse {
  width: 12px;
  height: 12px;
  border-radius: 50%;
  background: #ef4444;
  animation: pulse 1.5s infinite;
  flex-shrink: 0;
}

.recording-duration {
  font-size: 0.875rem;
  font-weight: 700;
  color: #1f2937;
  min-width: 45px;
  text-align: center;
}

.recording-status {
  font-size: 0.75rem;
  color: #6b7280;
  font-weight: 500;
}

/* Removed old small button styles - now using full-width button */

/* Animations */
@keyframes slideUp {
  from {
    opacity: 0;
    transform: translateY(1rem);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes slideUpFade {
  from {
    opacity: 0;
    transform: translateY(0.5rem);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes messageSlide {
  from {
    opacity: 0;
    transform: translateY(0.5rem);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes typing {
  0%, 60%, 100% {
    transform: translateY(0);
  }
  30% {
    transform: translateY(-0.5rem);
  }
}

@keyframes pulse {
  0%, 100% {
    opacity: 1;
    transform: scale(1);
  }
  50% {
    opacity: 0.8;
    transform: scale(1.05);
  }
}

@keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}

/* ==================== REALTIME MODE STYLES ==================== */

.realtime-container {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
  background: linear-gradient(180deg, #f9fafb 0%, #ffffff 100%);
}

/* Status Banner */
.realtime-status-banner {
  padding: 0.75rem 1rem;
  background: #f3f4f6;
  border-bottom: 1px solid #e5e7eb;
  flex-shrink: 0;
}

.realtime-status-banner.status-connected {
  background: linear-gradient(135deg, #ecfdf5, #d1fae5);
  border-bottom-color: #10b981;
}

.realtime-status-banner.status-connecting {
  background: linear-gradient(135deg, #eff6ff, #dbeafe);
  border-bottom-color: #3b82f6;
}

.realtime-status-banner.status-error {
  background: linear-gradient(135deg, #fef2f2, #fee2e2);
  border-bottom-color: #ef4444;
}

.status-indicator {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  justify-content: center;
}

.status-dot {
  width: 0.5rem;
  height: 0.5rem;
  border-radius: 50%;
  background: #6b7280;
}

.status-connected .status-dot {
  background: #10b981;
  animation: pulse 2s ease-in-out infinite;
}

.status-connecting .status-dot {
  background: #3b82f6;
  animation: pulse 1s ease-in-out infinite;
}

.status-error .status-dot {
  background: #ef4444;
}

.status-text {
  font-size: 0.75rem;
  font-weight: 500;
  color: #374151;
}

/* Realtime Content */
.realtime-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow-y: auto;
  padding: 2rem 1rem;
  gap: 2rem;
}

/* Avatar Section */
.realtime-avatar-section {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 1.5rem;
}

.realtime-avatar {
  position: relative;
  display: flex;
  align-items: center;
  justify-content: center;
}

.avatar-circle {
  width: 8rem;
  height: 8rem;
  border-radius: 50%;
  background: linear-gradient(135deg, #e5e7eb, #d1d5db);
  display: flex;
  align-items: center;
  justify-content: center;
  box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;
  position: relative;
  z-index: 2;
}

.realtime-avatar.connecting .avatar-circle {
  background: linear-gradient(135deg, #dbeafe, #93c5fd);
  animation: pulse 1.5s ease-in-out infinite;
}

.realtime-avatar.listening .avatar-circle {
  background: linear-gradient(135deg, #d1fae5, #6ee7b7);
  box-shadow: 0 0 0 0 rgba(16, 185, 129, 0.4);
  animation: realtimePulse 2s ease-in-out infinite;
}

.realtime-avatar.speaking .avatar-circle {
  background: linear-gradient(135deg, #bbf7d0, #86efac);
  box-shadow: 0 0 0 0 rgba(16, 185, 129, 0.6);
  animation: realtimePulse 1s ease-in-out infinite;
}

@keyframes realtimePulse {
  0%, 100% {
    transform: scale(1);
    box-shadow: 0 0 0 0 rgba(16, 185, 129, 0.4);
  }
  50% {
    transform: scale(1.05);
    box-shadow: 0 0 0 20px rgba(16, 185, 129, 0);
  }
}

.avatar-icon {
  font-size: 3rem;
  color: white;
}

/* Waveform Container */
.waveform-container {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 12rem;
  height: 12rem;
  display: flex;
  align-items: center;
  justify-content: center;
  pointer-events: none;
}

.waveform-bars {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.2rem;
  height: 4rem;
}

.waveform-bar {
  width: 0.25rem;
  background: linear-gradient(180deg, #10b981, #059669);
  border-radius: 0.125rem;
  animation: waveformPulse 1s ease-in-out infinite;
  opacity: 0.7;
}

@keyframes waveformPulse {
  0%, 100% {
    height: 0.5rem;
  }
  50% {
    height: 3rem;
  }
}

/* Status Text */
.realtime-status-text {
  text-align: center;
}

.realtime-status-text h3 {
  font-size: 1.25rem;
  font-weight: 600;
  color: #111827;
  margin: 0;
}

/* Transcript */
.realtime-transcript {
  flex: 1;
  background: white;
  border-radius: 0.75rem;
  padding: 1rem;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  overflow-y: auto;
  min-height: 8rem;
  max-height: 20rem;
}

.transcript-placeholder {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 100%;
  color: #9ca3af;
  font-size: 0.875rem;
  text-align: center;
}

.transcript-messages {
  display: flex;
  flex-direction: column;
  gap: 0.75rem;
}

.transcript-message {
  display: flex;
  gap: 0.5rem;
  font-size: 0.875rem;
  line-height: 1.5;
}

.transcript-role {
  font-weight: 600;
  color: #374151;
  flex-shrink: 0;
}

.transcript-message.user .transcript-role {
  color: #3b82f6;
}

.transcript-message.assistant .transcript-role {
  color: #10b981;
}

.transcript-content {
  color: #111827;
}

/* Realtime Controls */
.realtime-controls {
  padding: 1rem;
  background: white;
  border-top: 1px solid #e5e7eb;
  flex-shrink: 0;
}

.realtime-connect-button {
  width: 100%;
  padding: 1rem;
  background: linear-gradient(135deg, #10b981, #059669);
  border: none;
  border-radius: 0.75rem;
  color: white;
  font-size: 1rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.75rem;
  cursor: pointer;
  transition: all 0.2s;
  box-shadow: 0 4px 6px -1px rgba(16, 185, 129, 0.3);
}

.realtime-connect-button:hover:not(:disabled) {
  transform: translateY(-2px);
  box-shadow: 0 6px 12px -2px rgba(16, 185, 129, 0.4);
}

.realtime-connect-button:active:not(:disabled) {
  transform: translateY(0);
}

.realtime-connect-button:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

.realtime-connect-button i {
  font-size: 1.25rem;
}

/* Talk Controls */
.realtime-talk-controls {
  display: flex;
  flex-direction: column;
  gap: 0.75rem;
}

.realtime-disconnect-button {
  width: 100%;
  padding: 1rem;
  background: linear-gradient(135deg, #ef4444, #dc2626);
  border: none;
  border-radius: 0.75rem;
  color: white;
  font-size: 1rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.75rem;
  cursor: pointer;
  transition: all 0.2s;
  box-shadow: 0 4px 6px -1px rgba(239, 68, 68, 0.3);
}

.realtime-disconnect-button:hover {
  transform: translateY(-2px);
  box-shadow: 0 6px 12px -2px rgba(239, 68, 68, 0.4);
}

.realtime-disconnect-button:active {
  transform: translateY(0);
}

.realtime-disconnect-button i {
  font-size: 1.25rem;
}

.realtime-mode-info {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.5rem;
  padding: 0.75rem;
  background: #eff6ff;
  border-radius: 0.5rem;
  font-size: 0.75rem;
  color: #1e40af;
}

.realtime-mode-info i {
  font-size: 0.875rem;
}
</style>

